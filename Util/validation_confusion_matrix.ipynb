{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurable parameter\n",
    "model_path = r'../InceptionResNet_v2/inceptionresnet_v2_subset2_cv0_shape480_12_epoch.h5'\n",
    "model_val_dir = r'D:\\Resources\\Inat_Partial\\Aves_Small_SS2_Validation\\CV_0'\n",
    "# Plotting the training might be useful\n",
    "# model_val_dir = r'D:\\Resources\\Inat_Partial\\Aves_Small_SS2_Train\\CV_0'\n",
    "# The size used during training\n",
    "target_size = (480,480)\n",
    "batch_size = 16\n",
    "# Whether to normalize the confusion matrix or not\n",
    "cf_norm = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generated variable that might be quite heavy, load it only once\n",
    "model = load_model(model_path)\n",
    "class_labels = os.listdir(model_val_dir)\n",
    "labels_to_num = {c:i for i,c in enumerate(class_labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_true_labels(test_dir):\n",
    "    \"\"\"\n",
    "    Generating the list of true label for each validation image in order\n",
    "    \"\"\"\n",
    "    true_labels = []\n",
    "    for i,subdir in enumerate(os.listdir(test_dir)):\n",
    "        true_labels += [i]*len(os.listdir(os.path.join(test_dir,subdir)))\n",
    "    return np.array(true_labels)\n",
    "\n",
    "true_labels = generate_true_labels(model_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        model_val_dir,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        shuffle=False) # Must not be shuffled so it can be compared with the true labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List array of prediction probability for each class\n",
    "# This cell is slow, don't rerun it unless necessary\n",
    "predictions = model.predict_generator(validation_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best prediction on each image\n",
    "y_pred = np.argmax(predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function from scikit learn example\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title, fontsize=20)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label', fontsize=16)\n",
    "    plt.xlabel('Predicted label', fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(true_labels, y_pred)\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(15,10))\n",
    "title = 'Confusion matrix, with normalization' if cf_norm else 'Confusion matrix, without normalization'\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_labels,\n",
    "                      normalize=cf_norm,\n",
    "                      title=title)\n",
    "plt.figure(figsize=(15,10))\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_labels,\n",
    "                      normalize=False,\n",
    "                      title='No norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the classification report to dataframe\n",
    "# https://stackoverflow.com/questions/39662398/scikit-learn-output-metrics-classification-report-into-csv-tab-delimited-format\n",
    "# Edited a bit to cater for the difference in format\n",
    "# Can't really get the last row (avg / total) to a proper format, don't use it for now\n",
    "def classifaction_report_df(report):\n",
    "    report_data = []\n",
    "    lines = report.split('\\n')\n",
    "    for line in lines[2:-3] + lines[-2:-1]:\n",
    "        row = {}\n",
    "        row_data = line.split()\n",
    "        row['class'] = ' '.join(row_data[:-4])\n",
    "        row['precision'] = float(row_data[-4])\n",
    "        row['recall'] = float(row_data[-3])\n",
    "        row['f1_score'] = float(row_data[-2])\n",
    "        row['support'] = int(row_data[-1])\n",
    "        report_data.append(row)\n",
    "    dataframe = pd.DataFrame.from_dict(report_data)\n",
    "    dataframe.set_index(dataframe.columns[0], inplace=True)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable warning just for this\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    report = classification_report(true_labels, y_pred, target_names=class_labels)\n",
    "    report_df = classifaction_report_df(report)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

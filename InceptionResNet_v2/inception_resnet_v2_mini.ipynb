{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.models import load_model\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changeable parameter\n",
    "TRAIN_DIR = r'D:\\Resources\\Inat_Partial\\Aves_Small_SS1_Train\\CV_0'\n",
    "# TRAIN_DIR = r'D:\\Resources\\Inat_Partial\\Aves_Small_SS1_Augmented10SameClass\\CV_0'\n",
    "VAL_DIR = r'D:\\Resources\\Inat_Partial\\Aves_Small_SS1_Validation\\CV_0'\n",
    "#BATCH_SIZE = 16\n",
    "# Need smaller batch for deeper convolution\n",
    "BATCH_SIZE = 8\n",
    "# Set this to 3 for the large dataset\n",
    "EPOCH_NUM = 20\n",
    "# Set this to 0 or 1 for extremely large dataset (e.g. augmented)\n",
    "CALLBACK_PATIENCE = 20\n",
    "INPUT_WIDTH = 480\n",
    "INPUT_HEIGHT = 480\n",
    "LOAD_PREV_MODEL = True\n",
    "FINE_TUNE_MODEL = True\n",
    "# The first layer in the model to unfreeze for the fine tuning\n",
    "# FINE_TUNE_LAYER = 'conv2d_181'\n",
    "FINE_TUNE_LAYER = 'conv2d_118'\n",
    "RANDOM_SEED = 5703\n",
    "# Make sure to change this to the designated model\n",
    "PREV_MODEL_PATH = r'inceptionresnetv2_subset1_cv0_shape480_38_epoch.h5'\n",
    "# PREV_MODEL_PATH = r'inceptionresnetv2_subset1_augmentsame_cv0_shape480_2_epoch.h5'\n",
    "PREV_FINE_TUNE_PATH = r'inceptionresnetv2_ft2_subset1_cv0_shape480_0_epoch.h5'\n",
    "MODEL_METRICS_PATH = r'inceptionresnetv2_ft2_subset1_cv0_shape480_history.csv'\n",
    "# MODEL_METRICS_PATH = r'inceptionresnetv2_ft2_subset1_augmentsame_cv0_shape480_history.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-changeable parameter\n",
    "cur_model_path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limiting the number of resources used\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "config.gpu_options.allow_growth = True\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default configuration from\n",
    "# https://keras.io/preprocessing/image/\n",
    "# With a little bit of change in parameter\n",
    "train_datagen = ImageDataGenerator(\n",
    "        preprocessing_function=preprocess_input,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "# Do not enable extra augmentation for already augmented dataset\n",
    "# train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        TRAIN_DIR,\n",
    "        target_size=(INPUT_HEIGHT, INPUT_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical', \n",
    "        seed=RANDOM_SEED)\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        VAL_DIR,\n",
    "        target_size=(INPUT_HEIGHT, INPUT_WIDTH),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not reusing any of the fully connected layer, \n",
    "# in case of resnet inception they go straight to the output after the global pooling however\n",
    "if LOAD_PREV_MODEL == True and FINE_TUNE_MODEL == False and os.path.isfile(PREV_MODEL_PATH) == True:\n",
    "    print('loading previous model')\n",
    "    model = load_model(PREV_MODEL_PATH)\n",
    "elif LOAD_PREV_MODEL == True and FINE_TUNE_MODEL == True and os.path.isfile(PREV_MODEL_PATH) == True:\n",
    "    # Try to load from the previous fine tuning model, if it does not exist, load the transfer learning model\n",
    "    print('fine tuning the model')\n",
    "    if os.path.isfile(PREV_FINE_TUNE_PATH) == True:\n",
    "        model = load_model(PREV_FINE_TUNE_PATH)\n",
    "    # Modify the layer freeze state\n",
    "    else:\n",
    "        model = load_model(PREV_MODEL_PATH)\n",
    "        # Unfreeze everything first\n",
    "        for layer in model.layers:\n",
    "            layer.trainable = True\n",
    "        # Then freeze all the layers back until the target layer\n",
    "        for layer in model.layers:\n",
    "            if layer.name == FINE_TUNE_LAYER:\n",
    "                break\n",
    "            layer.trainable = False\n",
    "        # Recompile the model with different lower learning rate optimizer\n",
    "        model.compile(optimizer=SGD(lr=1e-4, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    # Replace the path to be saved at the latter stage\n",
    "    PREV_MODEL_PATH = PREV_FINE_TUNE_PATH\n",
    "else:\n",
    "    print('no previous model')\n",
    "    base_model = InceptionResNetV2(include_top=False,weights='imagenet',input_shape=(INPUT_HEIGHT,INPUT_WIDTH,3))\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    # Disable the dense layer\n",
    "    x = Dense(1024, activation='relu')(x)  \n",
    "    predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "\n",
    "    # this is the model we will train\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "    # Freezing all the base model layers\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # and then compile the model\n",
    "    # Use the default optimizer is fine for the transfer learning stage\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numbering the current epoch based on the previous epoch path\n",
    "# Handling multiple re-run of the training cell as well\n",
    "split_prev_model_path = PREV_MODEL_PATH.split('_') if cur_model_path is None else cur_model_path.split('_')\n",
    "prev_epoch_number = int(split_prev_model_path[-2])\n",
    "cur_epoch_number = prev_epoch_number + EPOCH_NUM\n",
    "cur_model_path = '_'.join(split_prev_model_path[:-2]+[str(cur_epoch_number)]+split_prev_model_path[-1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING CELL\n",
    "num_train_samples = train_generator.samples\n",
    "train_epoch_steps = math.ceil(num_train_samples / BATCH_SIZE)\n",
    "num_val_samples = validation_generator.samples\n",
    "val_epoch_steps = math.ceil(num_val_samples / BATCH_SIZE)\n",
    "# Note: In case of early stopping, rename the saved file accordingly\n",
    "callbacks = [EarlyStopping(monitor='val_loss', patience=CALLBACK_PATIENCE, verbose=0, mode='auto'), \n",
    "             ModelCheckpoint(filepath=cur_model_path, verbose=1, save_best_only=True)]\n",
    "train_history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch=train_epoch_steps,\n",
    "                    epochs=EPOCH_NUM,\n",
    "                    validation_data=validation_generator,\n",
    "                    validation_steps=val_epoch_steps, \n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the train history in the designated csv file\n",
    "# Note: newline has to be '' due to the way csv writerow works\n",
    "with open(MODEL_METRICS_PATH, 'a+', newline='') as history_file:\n",
    "    csv_writer = csv.writer(history_file)\n",
    "    for e in train_history.epoch:\n",
    "        epoch_number = prev_epoch_number + e\n",
    "        csv_writer.writerow([epoch_number,\n",
    "                            train_history.history['acc'][e],\n",
    "                            train_history.history['loss'][e],\n",
    "                            train_history.history['val_acc'][e],\n",
    "                            train_history.history['val_loss'][e]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally save the model with the latest epoch result\n",
    "model.save(cur_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
